# GenAI RAG Backend (FastAPI)

## ğŸ“Œ Overview
This project is a backend service built using FastAPI.  
It will gradually evolve into a **GenAI RAG (Retrieval-Augmented Generation)** system that can answer questions from documents.

## ğŸ›  Tech Stack (Planned)
- Python
- FastAPI
- LangChain
- Vector Database (FAISS / Chroma)
- LLM (OpenAI / Llama)

## ğŸ“‚ Project Structure

genai-rag-fastapi/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”‚   â”œâ”€â”€ text_chunker.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ simple.pdf
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

## ğŸ“… Learning Goal
Build a production-style GenAI backend while learning step-by-step.

## ğŸš€Project Status

### Phase 0 â€“ Backend Setup âœ…
- Python virtual environment setup
- FastAPI backend initialized
- Health check endpoint
- Mock AI question endpoint

### Phase 1 â€“ Document Processing âœ…
- PDF ingestion completed
- Text extraction working
- Recursive text chunking implemented
- Sample PDF tested successfully
- Project structured for RAG pipeline

### Phase 2 â€“ Vector Search (In Progress ğŸš§)
- Generate embeddings for chunks
- Store embeddings in FAISS vector database
- Implement semantic search

## ğŸ›£ï¸ Roadmap
- [x] Initialize repository and project structure
- [x] Add FastAPI skeleton
- [x] Run FastAPI server locally
- [x] Add document ingestion
- [ ] Implement embeddings
- [ ] Add retrieval pipeline
- [ ] Integrate LLM for answer generation
- [ ] Add API documentation and screenshots

## â–¶ï¸ How to Run
1. Clone the repository

   git clone https://github.com/your-username/genai-rag-fastapi.git
   cd genai-rag-fastapi

2. Create virtual environment

   python -m venv venv

3. Activate virtual environment

   On Windows:
   venv\Scripts\activate

   On Mac/Linux:
   source venv/bin/activate

4. Install dependencies

   pip install -r requirements.txt

5. Run FastAPI server

   uvicorn app.main:app --reload

6. Open in browser

   http://127.0.0.1:8000/docs
